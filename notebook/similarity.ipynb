{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from evaluate import load\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"./\")\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def extract_filename(path):\n",
    "    \"\"\"Extract filename from path\"\"\"\n",
    "    return os.path.basename(path)\n",
    "\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Helper function to load jsonl files\"\"\"\n",
    "    with open(file_path) as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "def calculate_metrics(source_texts, ground_truths):\n",
    "    # Initialize metrics\n",
    "    bleu_metric = load(\"bleu\")\n",
    "    rouge_metric = load(\"rouge\")\n",
    "    meteor_metric = load(\"meteor\")\n",
    "\n",
    "    # Calculate metrics\n",
    "    bleu_results = bleu_metric.compute(\n",
    "        predictions=source_texts, references=ground_truths\n",
    "    )\n",
    "    rouge_results = rouge_metric.compute(\n",
    "        predictions=source_texts, references=ground_truths\n",
    "    )\n",
    "    meteor_results = meteor_metric.compute(\n",
    "        predictions=source_texts, references=ground_truths\n",
    "    )\n",
    "\n",
    "    # Extract relevant scores\n",
    "    return {\n",
    "        \"bleu\": bleu_results[\"bleu\"],\n",
    "        \"bleu1\": bleu_results[\"precisions\"][0],\n",
    "        \"bleu2\": bleu_results[\"precisions\"][1],\n",
    "        \"bleu3\": bleu_results[\"precisions\"][2],\n",
    "        \"bleu4\": bleu_results[\"precisions\"][3],\n",
    "        \"rouge\": rouge_results[\"rougeL\"],\n",
    "        \"meteor\": meteor_results[\"meteor\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_models(gt_path, pred_path):\n",
    "    # Load data\n",
    "    logger.info(f\"Loading ground truth from {gt_path}\")\n",
    "    gt_data = load_jsonl(gt_path)\n",
    "    logger.info(f\"Loaded {len(gt_data)} ground truth entries\")\n",
    "\n",
    "    logger.info(f\"Loading predictions from {pred_path}\")\n",
    "    pred_data = load_jsonl(pred_path)\n",
    "    logger.info(f\"Loaded {len(pred_data)} prediction entries\")\n",
    "\n",
    "    # Create mapping from filenames to ground truth\n",
    "    gt_mapping = {\n",
    "        (\n",
    "            extract_filename(item[\"pre_image\"]),\n",
    "            extract_filename(item[\"post_image\"]),\n",
    "        ): item[\"change_caption\"]\n",
    "        for item in gt_data\n",
    "    }\n",
    "    logger.info(f\"Created ground truth mapping with {len(gt_mapping)} entries\")\n",
    "\n",
    "    # Group predictions by model_id\n",
    "    model_predictions = defaultdict(lambda: {\"source_texts\": [], \"ground_truths\": []})\n",
    "    matched_count = 0\n",
    "\n",
    "    for pred in pred_data:\n",
    "        key = (\n",
    "            extract_filename(pred[\"pre_image\"]),\n",
    "            extract_filename(pred[\"post_image\"]),\n",
    "        )\n",
    "        if key in gt_mapping:\n",
    "            matched_count += 1\n",
    "            model_predictions[pred[\"model_id\"]][\"source_texts\"].append(\n",
    "                pred[\"change_caption\"]\n",
    "            )\n",
    "            model_predictions[pred[\"model_id\"]][\"ground_truths\"].append(gt_mapping[key])\n",
    "        else:\n",
    "            logger.debug(f\"No match found for: {key}\")\n",
    "\n",
    "    logger.info(f\"Matched {matched_count} predictions with ground truth\")\n",
    "    logger.info(f\"Found {len(model_predictions)} models in predictions\")\n",
    "\n",
    "    # Calculate metrics for each model\n",
    "    results = {}\n",
    "    for model_id, data in model_predictions.items():\n",
    "        logger.info(\n",
    "            f\"Calculating metrics for model {model_id} with {len(data['source_texts'])} samples\"\n",
    "        )\n",
    "        results[model_id] = calculate_metrics(\n",
    "            data[\"source_texts\"], data[\"ground_truths\"]\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = evaluate_models(\"../data/xbd/xbd_gt.jsonl\", \"../output/xbd_subset_baseline.jsonl\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g22338035czy/conda_envs/genai0108/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# model = SentenceTransformer('../models/sentence-transformers/sentence-t5-base')\n",
    "model = SentenceTransformer('../models/sentence-transformers/sentence-t5-xxl',device=\"cuda:1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2984977c10434592e9ac70f2cf1f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "(1, 768)\n",
      "[[0.58259344]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "embeddings = model.encode(sentences,normalize_embeddings=True, show_progress_bar=True)\n",
    "print(embeddings[0].shape)\n",
    "emb1 = np.expand_dims(embeddings[0], axis=0)  # Shape: (1, 768)\n",
    "emb2 = np.expand_dims(embeddings[1], axis=0)  # Shape: (1, 768)\n",
    "print(emb1.shape)\n",
    "\n",
    "scs = abs((cosine_similarity(emb1, emb2)) ** 3)\n",
    "print(scs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading ground truth from ../data/xbd/xbd_gt.jsonl\n",
      "INFO:__main__:Loaded 44136 ground truth entries\n",
      "INFO:__main__:Loading predictions from ../output/xbd_subset_baseline.jsonl\n",
      "INFO:__main__:Loaded 5928 prediction entries\n",
      "INFO:__main__:Created ground truth mapping with 44136 entries\n",
      "INFO:__main__:Matched 5928 predictions with ground truth\n",
      "INFO:__main__:Found 6 models in predictions\n",
      "INFO:__main__:Calculating metrics for model Qwen/Qwen2-VL-7B-Instruct with 988 samples\n",
      "INFO:__main__:Calculating metrics for model Salesforce/xgen-mm-phi3-mini-instruct-interleave-r-v1.5 with 988 samples\n",
      "INFO:__main__:Calculating metrics for model OpenGVLab/InternVL2_5-8B with 988 samples\n",
      "INFO:__main__:Calculating metrics for model llava-hf/llava-interleave-qwen-7b-hf with 988 samples\n",
      "INFO:__main__:Calculating metrics for model llava-hf/llava-onevision-qwen2-7b-ov-hf with 988 samples\n",
      "INFO:__main__:Calculating metrics for model mistralai/Pixtral-12B-2409 with 988 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'Qwen/Qwen2-VL-7B-Instruct': {'avg_word_count': 82.12246963562752},\n",
      "    'Salesforce/xgen-mm-phi3-mini-instruct-interleave-r-v1.5': {'avg_word_count': 61.5},\n",
      "    'OpenGVLab/InternVL2_5-8B': {'avg_word_count': 83.03542510121457},\n",
      "    'llava-hf/llava-interleave-qwen-7b-hf': {'avg_word_count': 69.00708502024291},\n",
      "    'llava-hf/llava-onevision-qwen2-7b-ov-hf': {'avg_word_count': 75.99392712550608},\n",
      "    'mistralai/Pixtral-12B-2409': {'avg_word_count': 87.87854251012146}}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"Helper function to load jsonl files\"\"\"\n",
    "    with open(file_path) as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "\n",
    "def extract_filename(path):\n",
    "    \"\"\"Extract filename from path\"\"\"\n",
    "    return os.path.basename(path)\n",
    "   \n",
    "def calculate_cosine_similarity(source_texts, ground_truths):\n",
    "    # Get embeddings for both sets of texts\n",
    "    source_embeddings = model.encode(source_texts, normalize_embeddings=True)\n",
    "    gt_embeddings = model.encode(ground_truths, normalize_embeddings=True)\n",
    "    \n",
    "    # Calculate cosine similarity for each pair\n",
    "    similarities = []\n",
    "    for src_emb, gt_emb in zip(source_embeddings, gt_embeddings):\n",
    "        src_emb = np.expand_dims(src_emb, axis=0)\n",
    "        gt_emb = np.expand_dims(gt_emb, axis=0)\n",
    "        similarity = abs(cosine_similarity(src_emb, gt_emb)[0][0] ** 3)\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    # Return average similarity\n",
    "    return np.mean(similarities)\n",
    "\n",
    "def evaluate_models(gt_path, pred_path):\n",
    "    # Load data using existing load_jsonl function\n",
    "    logger.info(f\"Loading ground truth from {gt_path}\")\n",
    "    gt_data = load_jsonl(gt_path)\n",
    "    logger.info(f\"Loaded {len(gt_data)} ground truth entries\")\n",
    "\n",
    "    logger.info(f\"Loading predictions from {pred_path}\")\n",
    "    pred_data = load_jsonl(pred_path)\n",
    "    logger.info(f\"Loaded {len(pred_data)} prediction entries\")\n",
    "\n",
    "    # Create mapping from filenames to ground truth\n",
    "    gt_mapping = {\n",
    "        (\n",
    "            extract_filename(item[\"pre_image\"]),\n",
    "            extract_filename(item[\"post_image\"]),\n",
    "        ): item[\"change_caption\"]\n",
    "        for item in gt_data\n",
    "    }\n",
    "    logger.info(f\"Created ground truth mapping with {len(gt_mapping)} entries\")\n",
    "\n",
    "    # Group predictions by model_id\n",
    "    model_predictions = defaultdict(lambda: {\"source_texts\": [], \"ground_truths\": []})\n",
    "    matched_count = 0\n",
    "\n",
    "    for pred in pred_data:\n",
    "        key = (\n",
    "            extract_filename(pred[\"pre_image\"]),\n",
    "            extract_filename(pred[\"post_image\"]),\n",
    "        )\n",
    "        if key in gt_mapping:\n",
    "            matched_count += 1\n",
    "            model_predictions[pred[\"model_id\"]][\"source_texts\"].append(\n",
    "                pred[\"change_caption\"]\n",
    "            )\n",
    "            model_predictions[pred[\"model_id\"]][\"ground_truths\"].append(gt_mapping[key])\n",
    "        else:\n",
    "            logger.debug(f\"No match found for: {key}\")\n",
    "\n",
    "    logger.info(f\"Matched {matched_count} predictions with ground truth\")\n",
    "    logger.info(f\"Found {len(model_predictions)} models in predictions\")\n",
    "\n",
    "    # Calculate metrics for each model\n",
    "    results = {}\n",
    "    for model_id, data in model_predictions.items():\n",
    "        logger.info(\n",
    "            f\"Calculating metrics for model {model_id} with {len(data['source_texts'])} samples\"\n",
    "        )\n",
    "        # Calculate average caption length\n",
    "        total_words = sum(len(caption.split()) for caption in data['source_texts'])\n",
    "        avg_word_count = total_words / len(data['source_texts'])\n",
    "        \n",
    "        # Create a new metrics dictionary for each model\n",
    "        metrics = {\n",
    "            # \"cosine_similarity\": calculate_cosine_similarity(\n",
    "            #     data[\"source_texts\"], data[\"ground_truths\"]\n",
    "            # ),\n",
    "            \"avg_word_count\": avg_word_count  # Add average length metric\n",
    "        }\n",
    "        results[model_id] = metrics\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_models(\"../data/xbd/xbd_gt.jsonl\", \"../output/xbd_subset_baseline.jsonl\")\n",
    "import pprint\n",
    "pprint.pprint(results, width=120, indent=4, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai0108",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
